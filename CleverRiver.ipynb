{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CleverRiver.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNyWWtEop3kmgfE6KrIToC1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mluppichini/CleverRiver/blob/main/CleverRiver.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='top'></a>\n",
        "\n",
        "# **CleverRiver**: an introduction\n",
        "\n",
        "CleverRiver is a Google Colab workflow to train deep learning models for the prediction of river flow.\n",
        "\n",
        "CleverRiver is developed by [Marco Luppichini](https://orcid.org/0000-0002-0913-3825), [Monica Bini](https://orcid.org/0000-0003-1482-2630) and [Roberto Giannecchini](https://orcid.org/0000-0003-0447-3086) of the Earth Sciences Department of the University of Pisa, Italy.\n",
        "\n",
        "For more information see the [following paper](https://link.springer.com/article/10.1007/s12145-022-00903-7)\n",
        "\n",
        "For more information on the activities of the research group click [here](http://labclimambiente.dst.unipi.it/)\n",
        "\n",
        "\n",
        "version: 1.1 <br>\n",
        "release date: 2024-01-23\n",
        "<br>\n",
        "<br>\n",
        "The worflow is composed by three sections:\n",
        "> Section I: in this section is possible to train deep learning models using different archictectures and export of the results\n",
        "\n",
        "> Section II: in this section is possible to create predictions and displaing the results with graphs and csv files\n",
        "\n",
        "> Section III: in this section is possible to send reports and suggestions to the research group\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n4-2yRhlB5K7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='sectionI'></a>\n",
        "# **Section I**\n",
        "\n"
      ],
      "metadata": {
        "id": "dI7qtApEZowo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Step 1.1**: Set the workspace\n",
        "creation of the **workspace** and the **directory** and import of the needed **libraries**\n",
        "\n"
      ],
      "metadata": {
        "id": "HGiM4nc6MRjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Execute Step 1.1\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import *\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import datetime\n",
        "from datetime import timedelta\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.initializers import RandomUniform\n",
        "from tensorflow.keras.layers import LeakyReLU, RNN\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.callbacks import History, ModelCheckpoint\n",
        "from time import time\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.regularizers import l2, l1_l2\n",
        "from tensorflow.keras import callbacks\n",
        "from tensorflow.keras.models import model_from_json\n",
        "import keras\n",
        "from math import sqrt\n",
        "import shutil\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import json\n",
        "\n",
        "def datetime_range(start, end, delta):\n",
        "    current = start\n",
        "    while current < end:\n",
        "        yield current\n",
        "        current += delta\n",
        "\n",
        "def save_deep_learning_model(model, output_model_json, save_weights = False):\n",
        "    model_json = model.to_json()\n",
        "    with open(output_model_json, \"w\") as json_file:\n",
        "        json_file.write(model_json)\n",
        "    # serialize weights to HDF5\n",
        "    if save_weights == True:\n",
        "        output_model_h5 = output_model_json.replace(\".json\", \".h5\")\n",
        "        model.save_weights(output_model_h5)\n",
        "\n",
        "def add_pre_post_series(df, n_step_back,step_t, n_step_forward):\n",
        "    df = df.copy()\n",
        "    if n_step_back >= 4:\n",
        "        n_step_back_res = 5\n",
        "    else:\n",
        "        n_step_back_res = n_step_back\n",
        "    for column in df.columns:\n",
        "        if column != \"datetime\":\n",
        "            for gg in range(1, n_step_back_res, 1):\n",
        "                nome_col_day = \"%s_%st\" % (column, gg)\n",
        "                df[nome_col_day] = df[column].shift(gg)\n",
        "            for gg in range(n_step_back_res + step_t - 1, n_step_back, step_t):\n",
        "                nome_col_day = \"%s_%st\" % (column, gg)\n",
        "                df[nome_col_day] = df[column].shift(gg)\n",
        "        if \"output\" in column:\n",
        "            for gg in range(1, n_step_forward):\n",
        "                nome_col_day = \"%s_%soutput_after\" % (column, gg)\n",
        "                df[nome_col_day] = df[column].shift(-gg)\n",
        "    return df\n",
        "\n",
        "\n",
        "def LEARNING_MODEL(df_tot,\n",
        "                   cnn_n_node, lstm_n_node,\n",
        "                  patience,\n",
        "                  folder_model_out ,\n",
        "                  date_max_train,\n",
        "                  date_max_validation,\n",
        "                  model_type,\n",
        "                  forcasting,\n",
        "                   optimizer,\n",
        "                   loss_function\n",
        "                  ):\n",
        "\n",
        "\n",
        "    if not os.path.isdir(folder_model_out): os.mkdir(folder_model_out)\n",
        "    folder_scatter_out = os.path.join(folder_model_out, 'scatters')\n",
        "    if not os.path.isdir(folder_scatter_out): os.mkdir(folder_scatter_out)\n",
        "    folder_history_out = os.path.join(folder_model_out, 'history')\n",
        "    if not os.path.isdir(folder_history_out): os.mkdir(folder_history_out)\n",
        "    folder_time_out = os.path.join(folder_model_out, 'time')\n",
        "    if not os.path.isdir(folder_time_out): os.mkdir(folder_time_out)\n",
        "\n",
        "    colonna_output = 'output'\n",
        "    df_tot = df_tot.sort_values('datetime')\n",
        "\n",
        "    if forcasting > 0:\n",
        "      df_tot[colonna_output] = df_tot[colonna_output].shift(-forcasting)\n",
        "    df_tot = df_tot.dropna()\n",
        "    #TRAIN\n",
        "    dfTrain = df_tot[df_tot['datetime'] <= date_max_train]\n",
        "    X_train = dfTrain.drop([colonna_output, 'datetime'], axis=1).values\n",
        "    y_train = dfTrain[colonna_output].values\n",
        "\n",
        "    #VALIDATION\n",
        "    dfVal = df_tot[(df_tot['datetime'] > date_max_train) & (df_tot['datetime'] <= date_max_validation)]\n",
        "    X_val = dfVal.drop([colonna_output, 'datetime'], axis=1).values\n",
        "    y_val = dfVal[colonna_output].values\n",
        "\n",
        "    #TEST\n",
        "    dfTest = df_tot[df_tot['datetime'] > date_max_validation]\n",
        "    X_test = dfTest.drop([colonna_output, 'datetime'], axis=1).values\n",
        "    y_test = dfTest[colonna_output].values\n",
        "\n",
        "    bst_model_path = folder_model_out + os.sep + \"%st.h5\" % (forcasting)\n",
        "    bst_model_path_json = folder_model_out + os.sep + \"%st.json\" % (forcasting)\n",
        "\n",
        "    # if not os.path.isfile(folder_model_out + os.sep + \"Test.csv\"):\n",
        "    #     dfTest.to_csv(folder_model_out + os.sep +   \"Test.csv\", index=False)\n",
        "    # if not os.path.isfile(folder_model_out + os.sep   + \"_Train.csv\"):\n",
        "    #     dfTrain.to_csv(folder_model_out + os.sep   + \"_Train.csv\", index=False)\n",
        "    # if not os.path.isfile(folder_model_out + os.sep   + \"_Val.csv\"):\n",
        "    #     dfVal.to_csv(folder_model_out + os.sep   + \"_Val.csv\", index=False)\n",
        "\n",
        "    if model_type == 'CNN-LSTM':\n",
        "        model, X_train2, X_test2,y_train2, y_test2, history = LearningModelCNNLSTM(X_train, X_val, y_train, y_val, bst_model_path, bst_model_path_json, patience, cnn_n_node, lstm_n_node, optimizer, loss_function)\n",
        "    elif model_type == 'LSTM-ED':\n",
        "        model, X_train2, X_test2,y_train2, y_test2, history = LearningModelLSTMED(X_train, X_val, y_train, y_val, bst_model_path, bst_model_path_json, patience, lstm_n_node, optimizer, loss_function)\n",
        "    elif model_type == 'LSTM':\n",
        "        model, X_train2, X_test2, y_train2, y_test2, history = LearningModelLSTM(X_train, X_val, y_train, y_val,\n",
        "                                                                                  bst_model_path,\n",
        "                                                                                 bst_model_path_json, patience,\n",
        "                                                                                   lstm_n_node, optimizer, loss_function)\n",
        "    if 'CNN-LSTM' == model_type:\n",
        "        X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "    elif 'LSTM' == model_type:\n",
        "        X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "    elif 'LSTM-ED' == model_type:\n",
        "        X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "    #X_test = X_test.reshape((X_test.shape[0], 2, 2, 1))\n",
        "    yhat = model.predict(X_test.astype(np.float32), verbose=0)\n",
        "\n",
        "\n",
        "   # if  model_type == 'LSTM-ED': yhat = [i[0][0] for i in yhat ]\n",
        "\n",
        "    dfPred = pd.DataFrame()\n",
        "    dfPred['datetime'] = dfTest['datetime'].values\n",
        "    dfPred['observed'] = dfTest[colonna_output].values\n",
        "    dfPred['predicted'] = yhat\n",
        "    dfPred.to_csv(folder_model_out +os.sep+ str(forcasting) +\"_predicted.csv\", index = False)\n",
        "\n",
        "    # calculate RMSE\n",
        "    rmse = sqrt(mean_squared_error(y_test, yhat))\n",
        "    r2 = r2_score(y_test, yhat)\n",
        "    mae= mean_absolute_error(y_test, yhat)\n",
        "    print (\"RMSE\", rmse)\n",
        "    print (\"R2\", r2)\n",
        "\n",
        "    n_epoche = len(history.epoch)\n",
        "\n",
        "    figure, ax = plt.subplots()\n",
        "    ax.set_xlabel(\"observed\", fontsize=16)\n",
        "    ax.set_ylabel(\"predicted\", fontsize=16)\n",
        "    ax.scatter(y_test, yhat)\n",
        "    ax.plot(y_test, y_test, color=\"red\")\n",
        "    plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0, hspace=0)\n",
        "    figure.set_figheight(8)\n",
        "    figure.set_figwidth(8)\n",
        "    figure.savefig(folder_scatter_out + os.sep + str(forcasting) + \"t.png\", dpi=90)\n",
        "    plt.close()\n",
        "\n",
        "    figure, ax = plt.subplots()\n",
        "    ax.set_xlabel(\"epoch\", fontsize=16)\n",
        "    ax.set_ylabel(\"loss\", fontsize=16)\n",
        "    ax.plot(history.history['loss'], label = \"loss\")\n",
        "    ax.plot(history.history['val_loss'], label = \"val_loss\")\n",
        "    ax.legend()\n",
        "    plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0, hspace=0)\n",
        "    figure.set_figheight(8)\n",
        "    figure.set_figwidth(8)\n",
        "    figure.savefig(folder_history_out + os.sep + str(forcasting) + \"t.png\",\n",
        "                    dpi=90)\n",
        "    plt.close()\n",
        "\n",
        "    figure, ax = plt.subplots()\n",
        "    ax.set_xlabel(\"time\", fontsize=16)\n",
        "\n",
        "    pass_timeDelta = dfTest['datetime'].iloc[1] - dfTest['datetime'].iloc[0]\n",
        "    delta = timedelta(days=pass_timeDelta.days)\n",
        "    ax.plot(dfTest['datetime'] + delta * forcasting, y_test, label=\"test\")\n",
        "    ax.plot(dfTest['datetime'] + delta * forcasting, yhat, label=\"predicted\")\n",
        "    ax.legend()\n",
        "    plt.subplots_adjust(left=0.1, bottom=0.1, right=0.95, top=0.9, wspace=0, hspace=0)\n",
        "    figure.set_figheight(6)\n",
        "    figure.set_figwidth(10)\n",
        "    figure.savefig(folder_time_out + os.sep +  str(forcasting) + \"t.png\",\n",
        "                    dpi=90)\n",
        "    plt.close()\n",
        "\n",
        "    df = pd.DataFrame()\n",
        "    df['Number of CNN nodes'] = [cnn_n_node]\n",
        "    df['Number of LSTM nodes'] = [lstm_n_node]\n",
        "    df['Number of epochs'] = [n_epoche]\n",
        "    df['RMSE'] = [rmse]\n",
        "    df['R2'] = [r2]\n",
        "    df['MAE'] = [mae]\n",
        "    df['Lenght of dataset'] = len(df_tot)\n",
        "    df['Length of train dataset'] = X_train.shape[0]\n",
        "    df['Length of validation dataset'] = X_val.shape[0]\n",
        "    df['Length of test dataset'] = X_test.shape[0]\n",
        "    df.to_csv(folder_model_out +os.sep+ str(forcasting) +\"t_metrics.csv\", index = False)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def LearningModelCNNLSTM(X_train, X_test, y_train, y_test, bst_model_path, bst_model_path_json, patience, cnn_n_node, lstm_n_node, optimizer, loss_function):\n",
        "    n_features = 1\n",
        "\n",
        "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], n_features))\n",
        "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], n_features))\n",
        "\n",
        "    # design network\n",
        "    rmse_lis, r2_list, number_nodi, n_epoche = [], [], [], []\n",
        "    earlystopping = callbacks.EarlyStopping(monitor=\"val_loss\",\n",
        "                                            mode=\"min\", patience=patience,\n",
        "                                            restore_best_weights=True)\n",
        "\n",
        "    model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=cnn_n_node, kernel_size=2, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(Conv1D(filters=cnn_n_node * 2, kernel_size=2))\n",
        "    model.add(MaxPooling1D(pool_size=1))\n",
        "    model.add(LSTM(lstm_n_node))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(50))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=optimizer.lower(), loss=loss_function.lower())\n",
        "    model.summary()\n",
        "    history = model.fit(X_train.astype(np.float32), y_train.astype(np.float32), epochs=10000, batch_size= 70*24*4,\n",
        "                        validation_data=(X_test.astype(np.float32), y_test.astype(np.float32)), verbose=0,\n",
        "                        shuffle=False, validation_split=0, callbacks=[earlystopping, model_checkpoint])\n",
        "\n",
        "    model.load_weights(bst_model_path)\n",
        "    save_deep_learning_model(model, bst_model_path_json, False)\n",
        "    keras.backend.clear_session()\n",
        "    return model, X_train, X_test, y_train, y_test, history\n",
        "\n",
        "\n",
        "def LearningModelLSTMED(X_train, X_test, y_train, y_test, bst_model_path, bst_model_path_json, patience, lstm_n_node, optimizer, loss_function):\n",
        "    n_features = 1\n",
        "    X_train = X_train.reshape((X_train.shape[0],  n_features, X_train.shape[1]))\n",
        "    X_test = X_test.reshape((X_test.shape[0], n_features,  X_test.shape[1]))\n",
        "\n",
        "    # design network\n",
        "    rmse_lis, r2_list, number_nodi, n_epoche = [], [], [], []\n",
        "    earlystopping = callbacks.EarlyStopping(monitor=\"val_loss\",\n",
        "                                            mode=\"min\", patience=patience,\n",
        "                                            restore_best_weights=True)\n",
        "\n",
        "    model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
        "\n",
        "\n",
        "    model = Sequential()\n",
        "    #Encoder\n",
        "    model.add(LSTM(lstm_n_node, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
        "    model.add(LSTM(int(lstm_n_node/2), activation='relu', return_sequences=False))\n",
        "    model.add(RepeatVector(X_train.shape[1]))\n",
        "    model.add(LSTM(int(lstm_n_node/2), activation='relu', return_sequences=True))\n",
        "    model.add(LSTM(lstm_n_node, activation='relu', return_sequences=True))\n",
        "    model.add(TimeDistributed(Dense(n_features)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(50))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=optimizer.lower(), loss= loss_function.lower())\n",
        "\n",
        "    model.summary()\n",
        "    history = model.fit(X_train.astype(np.float32), y_train.astype(np.float32), epochs=10000, batch_size=70*24*4,\n",
        "                        validation_data=(X_test.astype(np.float32), y_test.astype(np.float32)), verbose=0,\n",
        "                        shuffle=False, validation_split=0, callbacks=[earlystopping, model_checkpoint])\n",
        "\n",
        "    model.load_weights(bst_model_path)\n",
        "    save_deep_learning_model(model, bst_model_path_json, False)\n",
        "    keras.backend.clear_session()\n",
        "    return model, X_train, X_test, y_train, y_test, history\n",
        "\n",
        "\n",
        "def LearningModelLSTM(X_train, X_test, y_train, y_test, bst_model_path, bst_model_path_json, patience, lstm_n_node, optimizer, loss_function):\n",
        "    n_features = 1\n",
        "    X_train = X_train.reshape((X_train.shape[0],  n_features, X_train.shape[1]))\n",
        "    X_test = X_test.reshape((X_test.shape[0], n_features,  X_test.shape[1]))\n",
        "\n",
        "    # design network\n",
        "    rmse_lis, r2_list, number_nodi, n_epoche = [], [], [], []\n",
        "    earlystopping = callbacks.EarlyStopping(monitor=\"val_loss\",\n",
        "                                            mode=\"min\", patience=patience,\n",
        "                                            restore_best_weights=True)\n",
        "\n",
        "    model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(lstm_n_node, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(50))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=optimizer.lower(), loss=loss_function.lower())\n",
        "    model.summary()\n",
        "    history = model.fit(X_train.astype(np.float32), y_train.astype(np.float32), epochs=10000, batch_size=70*24*4,\n",
        "                        validation_data=(X_test.astype(np.float32), y_test.astype(np.float32)), verbose=0,\n",
        "                        shuffle=False, validation_split=0, callbacks=[earlystopping, model_checkpoint])\n",
        "\n",
        "    model.load_weights(bst_model_path)\n",
        "    save_deep_learning_model(model, bst_model_path_json, False)\n",
        "    keras.backend.clear_session()\n",
        "    return model, X_train, X_test, y_train, y_test, history\n",
        "\n",
        "\n",
        "folder_input = \"/content/training_input_data/\"\n",
        "folder_output = \"/content/trained_models/\"\n",
        "file_zip_out = \"/content/LearningModels\"\n",
        "file_output = 'output.csv'\n",
        "columns = ['datetime', 'value']\n",
        "col_out = 'output'\n",
        "\n",
        "if not os.path.isdir(folder_input): os.mkdir(folder_input)\n",
        "#if not os.path.isdir(folder_output): os.mkdir(folder_output)\n",
        "\n",
        "print (\"Ok all configurated!!\")"
      ],
      "metadata": {
        "id": "b3GwqAO0M6Bf",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "##**Step 1.2**: Import the data\n",
        "Import manually the input data in CSV format into the folder \"input_data\". If you do not see the folder, use the reload button. The input files must have two columns: 'datetime' and 'value'. Each csv must have a time series of a station. The CSV files of the model output have to be inserted in the \"training_input_data\" folder and called \"output.csv\". The column separator must be the comma and the decimal separator the dot. Then press the run button to check the imported data.\n",
        "\n"
      ],
      "metadata": {
        "id": "PRjth2Nf9xsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Execute Step 1.2\n",
        "ck_out = False\n",
        "c = 0\n",
        "dict_dati = {}\n",
        "sequence_files = []\n",
        "for r, d, f in os.walk(folder_input):\n",
        "  for file in f:\n",
        "      c+=1\n",
        "      if file == file_output:\n",
        "        ck_out = True\n",
        "      path = os.path.join(r, file)\n",
        "      df = pd.read_csv(path)\n",
        "      if list(df.columns)!= columns:\n",
        "        print (\"ERROR:\", file, \"wrong columns\")\n",
        "        break\n",
        "      df[columns[0]] = pd.to_datetime(df[columns[0]])\n",
        "      if file != file_output:\n",
        "        dict_dati[file] = df.copy()\n",
        "      else:\n",
        "        df.columns = [columns[0], col_out]\n",
        "        dict_dati[\"output\"] = df.copy()\n",
        "      sequence_files.append(file)\n",
        "if c == 0:\n",
        "  print (\"ERROR: there are not files in input_data directory\")\n",
        "elif ck_out == False:\n",
        "  print (\"ERROR: there is not file 'output.csv' in input_data directory\")\n",
        "else:\n",
        "  print (\"All files have been loaded correctly\")"
      ],
      "metadata": {
        "id": "tmIfEscr6cNV",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "##**Step 1.3**: Set parameters for the matrix input\n",
        "Define the number of **back shift** (variable *n_step_back*) and the **frequency** of step to give (variable *n_step*) .\n",
        "For example for 15 minutes frequency time series, we suggest **96 step back** (24 h x 4 times) with a **step of 4** (one each hour).\n",
        "\n",
        "For more information see the equation 1 in Luppichini et al. 2022:\n",
        "\n",
        "H = f(X_t) = f(H_(t-1),H_(t-2), H_(t-n),..., R_(t-1), R_(t-2), ... H_(t-n))\n",
        "\n",
        "where H stands for the predicted hydrometric height at time t; H_(t−1), H_(t−2),…, H(t−n) are the antecedent hydrometric heights (up to t–1, t–2,…, t–n time steps); R_(t−1), R_(t−2), R_(t−m) are the antecedent rainfall (t–1, t–2, …, t–m time steps).\n",
        "\n",
        "Some tests and trials highlighted the need to create an input datasetwith t up to 96 steps. To decrease the noise contained by many steps and close measurements, we provided every t for the first previous hour and then one every 4 steps (e.g., t-0, t-1, t-2, t-3, t-4, t-8, t-12, t-16,…, t-96) up to the 24th hour."
      ],
      "metadata": {
        "id": "HEP_y6TpQ_sQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "n_step_back = 24 * 4  # 24 hours  * 4 step --> 96 step back\n",
        "n_step = 4   # get a step each 4 times\n",
        "'''\n",
        "\n",
        "#@markdown Set parameters\n",
        "n_step_back = 24 #@param {type:\"number\"}\n",
        "n_step =  1#@param {type:\"number\"}\n",
        "n_step_back = int(n_step_back)\n",
        "n_step = int(n_step)\n",
        "\n"
      ],
      "metadata": {
        "id": "WJ50VffoQ6U-",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "##**Step 1.4**:  Create the input matrix\n",
        "Create the input matrix to train the model"
      ],
      "metadata": {
        "id": "7lIfzltXJVEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown it may take a few minutes\n",
        "c = 0\n",
        "for file in sequence_files:\n",
        "    if file == file_output:\n",
        "      file = 'output'\n",
        "    df = dict_dati[file].copy()\n",
        "    df = df.sort_values(columns[0])\n",
        "    # delta = (df['datetime'] - df['datetime'].shift(+1)).median()\n",
        "    # time_series_complete = [dt.strftime('%Y-%m-%d %H:%M:%S') for dt in datetime_range(df[columns[0]].min(), df[columns[0]].max(),\n",
        "    # delta)]\n",
        "    # dfDataset = pd.DataFrame()\n",
        "    # dfDataset[columns[0]] = pd.to_datetime(time_series_complete)\n",
        "    # dfDataset = dfDataset.join(df.set_index(columns[0]), on= columns[0])\n",
        "    # df = df.dropna()\n",
        "    if c == 0:\n",
        "      dfJoin = df.copy()\n",
        "    else:\n",
        "      dfJoin = dfJoin.join(df.set_index(columns[0]), on = columns[0], rsuffix = \"_%s\" %c)\n",
        "    c+=1\n",
        "dfJoin = add_pre_post_series(dfJoin, n_step_back, n_step, 0)\n",
        "number_nan = dfJoin.isna().sum().sum()\n",
        "#dfJoin = dfJoin.fillna(0)\n",
        "dfJoin = dfJoin.sort_values(columns[0])\n",
        "len_old = len(dfJoin)\n",
        "dfJoin = dfJoin.dropna()\n",
        "dfJoin = dfJoin.reset_index()\n",
        "dfJoin = dfJoin.drop('index', axis = 1)\n",
        "columns_delete = []\n",
        "for column in dfJoin.columns:\n",
        "    if 'datetime' in column and column != 'datetime': columns_delete.append(column)\n",
        "dfJoin = dfJoin.drop(columns_delete, axis = 1)\n",
        "print (\"Information input matrix:\")\n",
        "print (\"Length of time series:\", len(dfJoin) )\n",
        "print (\"Records eliminated for missing values: \" , len_old - len(dfJoin))\n",
        "print (\"N columns\", len(dfJoin.columns))\n",
        "print (\"N data in matrix:\", len(dfJoin) * (len(dfJoin.columns) -1))\n",
        "#print (\"N fill nan in matrix:\", number_nan)\n",
        "\n",
        "old_columns = dfJoin.columns\n",
        "\n"
      ],
      "metadata": {
        "id": "xiuEF8GiD2JW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "##**Step 1.5**: Set the parameters to train the models\n"
      ],
      "metadata": {
        "id": "SzXJpcYMONid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split of the input dataset** <br>\n",
        "The input dataset has to be divided in three part: train, validation and test dataset. The train and the validation dataset are used during the training procedures, while the test dataset is used during the evaluation of the results. Dividing the dataset allows to reduce the possibility to have overfitting.\n",
        "The partition 60%–20%–20% for training, validation and test dataset is used by several studies and permits to have sufficient data for the training and the evaluation of the model.\n",
        "\n",
        "\n",
        "**The model nodes** <br>\n",
        "Choose between three differtent archictures:\n",
        "1.   LSTM: a simple vanilla model\n",
        "2.   CNN-LSTM: a combination of CNN nodes and LSTM nodes\n",
        "3.   LSTM-ED: an encoder-decoder archicteture\n",
        "\n",
        "**The forecasting**\n",
        "<br>\n",
        "Choose the range of the prediction in relation of t. t is variable in relation of the data used. For example, if you use daily data, t = day. If you set max_t_forcasting to 10 and step_t to 2, the algorithm will calculate the predictions of 0 days, 2 days, 4 days, 6 days, 8 days, and 10 days.\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "For more **information**, we suggest to read the following works:\n",
        "*    [Luppichini, M., Barsanti, M., Giannecchini, R., & Bini, M. (2022). Deep learning models to predict flood events in fast-flowing watersheds. Science of The Total Environment, 813, 151885.](https://doi.org/https://doi.org/10.1016/j.scitotenv.2021.151885)\n",
        "*   [Lupi A., Luppichini M., Barsanti M., Giannecchini R. (in press) Deep learning models to complete rainfall time series databases affected by missing or anomalous data. Earth Science Informatics. Submitted.]()\n",
        "\n",
        "*The default setting are as suggest by Luppichini et al. 2022.*\n"
      ],
      "metadata": {
        "id": "nrL7yF5wCVu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ####Set train, validation and test dataset sizes as integer percentage values  (e.g, 60)\n",
        "train_size = 60 #@param {type:\"number\"}\n",
        "val_size = 20 #@param {type:\"number\"}\n",
        "test_size = 20 #@param {type:\"number\"}\n",
        "\n",
        "sum_perc = train_size + val_size + test_size\n",
        "if sum_perc != 100:\n",
        "  print (\"ERROR, repeat this procedure, the sum of the percentages must be 100%\")\n",
        "else:\n",
        "  pass\n",
        "\n",
        "#@markdown **Model parameters**\n",
        "#@markdown >If the select model does not include CNN the number of CNN nodes will be set to 0\n",
        "\n",
        "model_select = \"LSTM-ED\" #@param [\"LSTM\", \"LSTM-ED\", \"CNN-LSTM\"] {type:\"string\"}\n",
        "cnn_n_node = 8 #@param {type:\"slider\", min:8, max:128, step:2}\n",
        "lstm_n_node = 32 #@param {type:\"slider\", min:8, max:128, step:2}\n",
        "patience = 100 #@param {type:\"slider\", min:10, max:1000, step:2}\n",
        "optimizer = \"Adam\" #@param [\"Adam\", \"SGD\", \"RMSprop\"] {type:\"string\"}\n",
        "loss_function = \"MSE\"#@param [\"MSE\", \"MAE\"] {type:\"string\"}\n",
        "#@markdown >Chose the forward prediction maximum t and the step between each prediction.\n",
        "max_t_forecasting =  24#@param {type:\"number\"}\n",
        "step_t =  1#@param {type:\"number\"}\n",
        "\n",
        "\n",
        "if 'CNN' not in model_select:\n",
        "  cnn_n_node = 0\n",
        "\n",
        "lim1 = int(len(dfJoin) * train_size /100)\n",
        "lim2 = lim1 + int(len(dfJoin) * val_size/100)\n",
        "date1 = dfJoin.iloc[lim1]['datetime']\n",
        "date2 = dfJoin.iloc[lim2]['datetime']\n",
        "\n",
        "print (\"All parameters are set correctly\")\n",
        "print (\"End date of train dataset:\", date1)\n",
        "print (\"End date of validation dataset:\", date2)"
      ],
      "metadata": {
        "id": "j9ZoLQiKJLIM",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "##**Step 1.6**: Train your models"
      ],
      "metadata": {
        "id": "96sjuW15ORbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@markdown it may take many minutes\n",
        "\n",
        "#I remove the folder_output if exist... each time, we can create only a sequence of models\n",
        "try:\n",
        "  shutil.rmtree(folder_output)\n",
        "except:\n",
        "  pass\n",
        "\n",
        "os.mkdir(folder_output)\n",
        "\n",
        "json_param = {}\n",
        "json_param['files_input'] = sequence_files\n",
        "json_param['parameters'] = {}\n",
        "json_param['parameters']['n_step_back'] = n_step_back\n",
        "json_param['parameters']['n_step'] = n_step\n",
        "json_param['parameters']['model'] = model_select\n",
        "with open(os.path.join(folder_output, \"parameters.json\"), 'w') as f:\n",
        "    json.dump(json_param, f)\n",
        "for forecasting_step in range (0, max_t_forecasting + 1, step_t):\n",
        "    folder = \"learning_%s_%st\"%(model_select, forecasting_step)\n",
        "    learning_folder = os.path.join(folder_output, folder)\n",
        "    if not os.path.isdir(learning_folder): os.mkdir(learning_folder)\n",
        "    print (\"Model forecasting: \", forecasting_step, \"t\")\n",
        "    dfJoin = dfJoin.sort_values('datetime')\n",
        "    LEARNING_MODEL(dfJoin,\n",
        "                          cnn_n_node=cnn_n_node,\n",
        "                          lstm_n_node=lstm_n_node,\n",
        "                          patience=patience,\n",
        "                          folder_model_out= learning_folder,\n",
        "                          date_max_train=date1,\n",
        "                          date_max_validation=date2,\n",
        "                          model_type= model_select,\n",
        "                          forcasting = forecasting_step,\n",
        "                   optimizer = optimizer,\n",
        "                   loss_function = loss_function\n",
        "                          )"
      ],
      "metadata": {
        "id": "nvG-gOIhHDeN",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "##**Step 1.7**: Export the trained models\n",
        "Zip *'trained_models'* in \"LearningModels.zip\", you will can download it and save it into your Pc for future elaborations"
      ],
      "metadata": {
        "id": "8Cl9sfeNXqRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Execute the Step 1.7\n",
        "shutil.make_archive(file_zip_out, 'zip', folder_output)"
      ],
      "metadata": {
        "id": "Osq3wZuGXp8N",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='sectionII'></a>\n",
        "# **Section II**"
      ],
      "metadata": {
        "id": "KPwmjOf0BYca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "##**Step 2.1**: Set the workspace\n",
        "creation of the **workspace** and the **directory** and import of the needed **libriaries**"
      ],
      "metadata": {
        "id": "xgEnQlgwaDFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Execute Step 2.1\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import *\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import datetime\n",
        "from datetime import timedelta\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.initializers import RandomUniform\n",
        "from tensorflow.keras.layers import LeakyReLU, RNN\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.callbacks import History, ModelCheckpoint\n",
        "from time import time\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.regularizers import l2, l1_l2\n",
        "from tensorflow.keras import callbacks\n",
        "from tensorflow.keras.models import model_from_json\n",
        "import keras\n",
        "import zipfile\n",
        "from math import sqrt\n",
        "import json\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "def datetime_range(start, end, delta):\n",
        "    current = start\n",
        "    while current < end:\n",
        "        yield current\n",
        "        current += delta\n",
        "\n",
        "def save_deep_learning_model(model, output_model_json, save_weights = False):\n",
        "    model_json = model.to_json()\n",
        "    with open(output_model_json, \"w\") as json_file:\n",
        "        json_file.write(model_json)\n",
        "    # serialize weights to HDF5\n",
        "    if save_weights == True:\n",
        "        output_model_h5 = output_model_json.replace(\".json\", \".h5\")\n",
        "        model.save_weights(output_model_h5)\n",
        "\n",
        "def add_pre_post_series(df, n_step_back,step_t, n_step_forward):\n",
        "    df = df.copy()\n",
        "    if n_step_back >= 4:\n",
        "        n_step_back_res = 5\n",
        "    else:\n",
        "        n_step_back_res = n_step_back\n",
        "    for column in df.columns:\n",
        "        if column != \"datetime\":\n",
        "            for gg in range(1, n_step_back_res, 1):\n",
        "                nome_col_day = \"%s_%st\" % (column, gg)\n",
        "                df[nome_col_day] = df[column].shift(gg)\n",
        "            for gg in range(n_step_back_res + step_t - 1, n_step_back, step_t):\n",
        "                nome_col_day = \"%s_%st\" % (column, gg)\n",
        "                df[nome_col_day] = df[column].shift(gg)\n",
        "        if \"output\" in column:\n",
        "            for gg in range(1, n_step_forward):\n",
        "                nome_col_day = \"%s_%soutput_after\" % (column, gg)\n",
        "                df[nome_col_day] = df[column].shift(-gg)\n",
        "    return df\n",
        "def getPredictedValues(folder):\n",
        "    dfResult = pd.DataFrame()\n",
        "    for r, d, f in os.walk(folder, False):\n",
        "        for file in f:\n",
        "          if not file.endswith(\".csv\"): continue\n",
        "          pred = file.split(\"_\")[0].split(\"_\")[0]\n",
        "          path = os.path.join(r, file)\n",
        "          df = pd.read_csv(path)\n",
        "          colonne = [\"%s_%s\" %(column, pred) for column in df.columns if column != 'datetime']\n",
        "          colonne.insert(0, 'datetime')\n",
        "          df.columns = colonne\n",
        "\n",
        "          if dfResult.empty:\n",
        "              dfResult = df.copy()\n",
        "          else:\n",
        "              dfResult = dfResult.join(df.set_index('datetime'), on = 'datetime', lsuffix=\"_%s\" %pred)\n",
        "\n",
        "\n",
        "    dfResult['datetime'] = pd.to_datetime(dfResult['datetime'])\n",
        "    return dfResult\n",
        "\n",
        "def PlotSimulation(date_start,date_max , step, ylabel, figure_out =''):\n",
        "    #\n",
        "    folder_all = '/content/output_prediction'\n",
        "    nome_file_metrics = r\"metrics_total.csv\"\n",
        "\n",
        "    data_event_start = datetime.datetime.strptime(date_start, \"%Y-%m-%d %H:%M\")\n",
        "    data_event_end = datetime.datetime.strptime(date_max, \"%Y-%m-%d %H:%M\")\n",
        "    diff_date = data_event_end - data_event_start\n",
        "    nome_file_metrics = r\"metrics_total.csv\"\n",
        "    data_event_max = datetime.datetime.strptime(date_max, \"%Y-%m-%d %H:%M\")\n",
        "    data_rain_min = data_event_max - timedelta(days=10)\n",
        "    data_rain_max = data_event_max + timedelta(days=5)\n",
        "    dfResult = getPredictedValues(\"/content/output_prediction\")\n",
        "\n",
        "    delta = (dfResult['datetime'] - dfResult['datetime'].shift(+1)).median()\n",
        "    hours = delta.seconds//3600\n",
        "    minutes = (delta.seconds//60)%60\n",
        "    days = delta.days\n",
        "    delta = hours + minutes/60 + days * 24\n",
        "    if step % delta != 0:\n",
        "      print (\"ERROR select the step must be a multiple of the data frequency (t)\")\n",
        "      return\n",
        "    fig, ax = plt.subplots()\n",
        "    fig.set_figwidth(14)\n",
        "    fig.set_figheight(8)\n",
        "    plt.subplots_adjust(left=0.1, right=0.95, top=0.97, bottom=0.05, hspace=0.2, wspace=0.3)\n",
        "\n",
        "\n",
        "    xmin_graph = data_event_start + timedelta(hours=  -data_event_start.hour)\n",
        "    xmax_graph = data_event_start + timedelta(days=2, hours=6 + 1)\n",
        "    curve = dfResult[(dfResult['datetime']  >= xmin_graph) & (dfResult['datetime'] <= xmax_graph)]['y_real_0t']\n",
        "    time_max = 24\n",
        "    dfPlot = dfResult[(dfResult['datetime'] >= data_event_start) & (dfResult['datetime'] <= data_event_end)]\n",
        "    ax.plot(dfPlot['datetime'], dfPlot['y_real_0t'], label=\"observed\", color='black')\n",
        "\n",
        "    data_event_start2 = data_event_start + timedelta(hours=0)\n",
        "\n",
        "\n",
        "    k = 0\n",
        "    while data_event_start2 < data_event_end:\n",
        "        data_event_start2 = data_event_start + timedelta(hours = step * k)\n",
        "        if data_event_start2 > data_event_end: continue\n",
        "        k+=1\n",
        "        dfSel = dfResult[dfResult['datetime'] == data_event_start2]\n",
        "        if dfSel.empty:\n",
        "          print ('Impossibile to execute the simulation starting from %s' %data_event_start2)\n",
        "          continue\n",
        "        array = []\n",
        "        times = []\n",
        "        for pred in preds:\n",
        "            array.append(dfSel['y_hat_%st' % pred].values[0])\n",
        "            times.append(data_event_start2 + timedelta(hours = delta * pred))\n",
        "        dfRis = pd.DataFrame()\n",
        "        dfRis['datetime'] = times\n",
        "        dfRis['values'] = array\n",
        "        dfRis = dfRis.sort_values('datetime')\n",
        "        title = \"After %s t\" % (k)\n",
        "        ax.plot(dfRis['datetime'], dfRis['values'], label=title, marker=\"o\", linestyle=\":\",\n",
        "                  ms=4)  # color = colors[pred].get_hex(\n",
        "    ax.set_title(\"Simulation %s to %s\" %(date_start, date_max), fontsize=20)\n",
        "    ax.grid()\n",
        "    ax.set_ylabel(ylabel, fontsize = 20)\n",
        "   # l = ax.legend(fontsize=12, borderpad=1, loc='upper right')\n",
        "    fig.savefig(figure_out)\n",
        "folder_input = \"/content/input_data_prediction/\"\n",
        "folder_models = \"/content/trained_models\"\n",
        "folder_output = \"/content/output_prediction/\"\n",
        "file_output = 'output.csv'\n",
        "columns = ['datetime', 'value']\n",
        "col_out = 'output'\n",
        "\n",
        "if not os.path.isdir(folder_input): os.mkdir(folder_input)\n",
        "if not os.path.isdir(folder_output): os.mkdir(folder_output)\n",
        "if not os.path.isdir(folder_models): os.mkdir(folder_models)\n",
        "\n",
        "print (\"Ok all configurated!!\")"
      ],
      "metadata": {
        "id": "VGFRl_99dkMp",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "##**Step 2.2**: Import the trained model (optional)\n",
        "If folder \"trained_models\" is empty, you load manually the zip file extracted by the procedure of *Section I* calling it \"LearningModels.zip\" and then execute this command to extract the folder."
      ],
      "metadata": {
        "id": "nZcpjrbFeaOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Execute Step 2.2\n",
        "\n",
        "if os.path.isfile(\"/content/LearningModels.zip\"):\n",
        "  with zipfile.ZipFile(\"/content/LearningModels.zip\", 'r') as zip_ref:\n",
        "      zip_ref.extractall(folder_models)\n",
        "  print (\"Done\")\n",
        "else:\n",
        "  print (\"File LearningModels.zip not found, insert in the project folder the file\")"
      ],
      "metadata": {
        "id": "bOh-oknBfIrj",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "##**Step 2.3**: Import the data\n",
        "Import manually the input data in CSV format into the folder *'input_data_prediction'*. If you don't see the folder, use the reload button. The input files must have two columns: *'datetime'* and *'value'*. The csv of the model output have to be insert in *'input_data_folder'* and called *'output.csv'*. The name of the files must be the same used to train the models. The column separator must be the comma and the decimal separator the dot. Then press the run button."
      ],
      "metadata": {
        "id": "3vn9WzQpeBNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Execute Step 2.3\n",
        "with open(os.path.join(folder_models, \"parameters.json\")) as f:\n",
        "   parameters = json.load(f)\n",
        "ck_out = False\n",
        "c = 0\n",
        "dict_dati = {}\n",
        "sequence_files = []\n",
        "for r, d, f in os.walk(folder_input):\n",
        "  for file in f:\n",
        "      c+=1\n",
        "      if file == file_output:\n",
        "        ck_out = True\n",
        "      path = os.path.join(r, file)\n",
        "      df = pd.read_csv(path)\n",
        "      if list(df.columns)!= columns:\n",
        "        print (\"ERROR:\", file, \"wrong columns\")\n",
        "        break\n",
        "      df[columns[0]] = pd.to_datetime(df[columns[0]])\n",
        "      if file != file_output:\n",
        "        dict_dati[file] = df.copy()\n",
        "      else:\n",
        "        df.columns = [columns[0], col_out]\n",
        "        dict_dati[\"output\"] = df.copy()\n",
        "      sequence_files.append(file)\n",
        "\n",
        "for file in parameters['files_input']:\n",
        "  if file not in sequence_files:\n",
        "    print (\"ERROR: not all files used to train the models are loaded in input_data_prediction, check also the name of the files\")\n",
        "    break\n",
        "\n",
        "if c == 0:\n",
        "  print (\"ERROR: not files in input_data_prediction directory\")\n",
        "elif ck_out == False:\n",
        "  print (\"ERROR: not file 'output.csv' in input_data_prediction directory\")\n",
        "else:\n",
        "  print (\"All files are loaded correctly\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tlOj06RneIP3",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Step 2.4**: Create the input matrix\n",
        "Create the input matrix the models (as done in Step 1.4) and calculate the predictions.\n",
        "It may take a few minutes..."
      ],
      "metadata": {
        "id": "vRhPKAI-qBhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Execute the Step 2.4\n",
        "c = 0\n",
        "for file in parameters['files_input']:\n",
        "  if file == file_output:\n",
        "    file = 'output'\n",
        "  df = dict_dati[file].copy()\n",
        "  df = df.sort_values(columns[0])\n",
        "  if c == 0:\n",
        "    dfJoin = df.copy()\n",
        "  else:\n",
        "    dfJoin = dfJoin.join(df.set_index(columns[0]), on = columns[0], rsuffix = \"_%s\" %c)\n",
        "  c+=1\n",
        "n_step_back = parameters[\"parameters\"][\"n_step_back\"]\n",
        "n_step = parameters[\"parameters\"][\"n_step\"]\n",
        "dfJoin = add_pre_post_series(dfJoin, n_step_back, n_step, 0)\n",
        "number_nan = dfJoin.isna().sum().sum()\n",
        "#dfJoin = dfJoin.fillna(0)\n",
        "dfJoin = dfJoin.sort_values(columns[0])\n",
        "len_old = len(dfJoin)\n",
        "dfJoin = dfJoin.dropna()\n",
        "dfJoin = dfJoin.reset_index()\n",
        "dfJoin = dfJoin.drop('index', axis = 1)\n",
        "columns_delete = []\n",
        "for column in dfJoin.columns:\n",
        "    if 'datetime' in column and column != 'datetime': columns_delete.append(column)\n",
        "dfJoin = dfJoin.drop(columns_delete, axis = 1)\n",
        "print (\"Information input matrix:\")\n",
        "print (\"Length of time series:\", len(dfJoin) )\n",
        "print (\"Records eliminated for missing values: \" , len_old - len(dfJoin))\n",
        "print (\"N columns\", len(dfJoin.columns))\n",
        "print (\"N data in matrix:\", len(dfJoin) * (len(dfJoin.columns) -1))\n",
        "#print (\"N fill nan in matrix:\", number_nan)\n",
        "\n",
        "folder = r\"/content/trained_models\"\n",
        "valori = []\n",
        "model_type = parameters['parameters']['model']\n",
        "\n",
        "preds = []\n",
        "for r, d, f in os.walk(folder):\n",
        "    ends = r.split(os.sep)[-1]\n",
        "    if 'learning' not in ends: continue\n",
        "    print(r)\n",
        "    split_folder = os.path.split(r)\n",
        "    nome_folder = split_folder[-1]\n",
        "    pred = int(nome_folder.split(\"_\")[-1].split(\"t\")[0])\n",
        "    preds.append(pred)\n",
        "    df = dfJoin.copy()\n",
        "    y_real = df['output'].values\n",
        "    df['output'] = df['output'].shift(-pred)\n",
        "    y = df[\"output\"].values\n",
        "    x = df.drop(['output', \"datetime\"], axis=1).values\n",
        "    model_json_name = \"%st.json\" % (pred)\n",
        "    model_h5_name = \"%st.h5\" % (pred)\n",
        "\n",
        "    if 'CNN-LSTM' == model_type:\n",
        "          X = x.reshape((x.shape[0], x.shape[1], 1))\n",
        "    elif 'LSTM' == model_type:\n",
        "        x = x.reshape((x.shape[0], 1, x.shape[1]))\n",
        "    elif 'LSTM-ED' == model_type:\n",
        "        x = x.reshape((x.shape[0], 1, x.shape[1]))\n",
        "\n",
        "    path_model_h5 = os.path.join(r, model_h5_name)\n",
        "    path_model_json = os.path.join(r, model_json_name)\n",
        "    print (path_model_h5, path_model_json)\n",
        "    json_file = open(path_model_json, 'r')\n",
        "    loaded_model_json = json_file.read()\n",
        "    json_file.close()\n",
        "    model = keras.models.model_from_json(loaded_model_json)\n",
        "    model.load_weights(path_model_h5)\n",
        "\n",
        "    #X_test = X_test.reshape((X_test.shape[0], 2, 2, 1))\n",
        "    yhat = model.predict(x.astype(np.float32), verbose=0)\n",
        "\n",
        "    #if  model_type == 'LSTM-ED': yhat = [i[0][0] for i in yhat ]\n",
        "\n",
        "    df1 = pd.DataFrame()\n",
        "    df1['datetime'] = df['datetime'].values\n",
        "    df1['y_real'] = y_real\n",
        "    df1['y'] = y\n",
        "    df1['y_hat'] = yhat\n",
        "    path_out = os.path.join(folder_output, \"%st_predicted.csv\" %pred)\n",
        "    df1.to_csv(path_out, index = False)\n"
      ],
      "metadata": {
        "id": "yDM5pl_TqF3A",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "##**Step 2.5**: Execute a simulation of an event\n",
        "Chose the start date and the end date to simulate the prediction. The graph is saved in the folder '/content/output_predictions/plots'"
      ],
      "metadata": {
        "id": "sZhlmQ-kiCet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### Date fields\n",
        "date_start = '2019-12-02' #@param {type:\"date\"}\n",
        "date_end = '2019-12-04' #@param {type:\"date\"}\n",
        "#@markdown ### Temporal Interval between each simulation in hours:\n",
        "step =  6#@param {type:\"number\"}\n",
        "#@markdown ###  Plot parameters\n",
        "#@markdown > You can use Latex syntax to integrate math symbols\n",
        "ylabel = \"Flow [m]\" #@param {type:\"string\"}\n",
        "\n",
        "folder_plots = os.path.join(folder_output, \"plots\")\n",
        "if not os.path.isdir(folder_plots): os.mkdir(folder_plots)\n",
        "plot_path = os.path.join(folder_plots, \"%s_%s.png\" %(date_start, date_end))\n",
        "\n",
        "PlotSimulation(date_start + ' 00:00', date_end + ' 00:00', step,ylabel, plot_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "-2kY79xMiBNE",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Step 2.6**: Export the output folder\n",
        "Export in a zip file the folder \"/content/output_prediction/\""
      ],
      "metadata": {
        "id": "RZmD6BQTG0wk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Execute the Step 2.6\n",
        "shutil.make_archive(folder_ot, 'zip', folder_output)"
      ],
      "metadata": {
        "id": "C6fWQHknG0Gj",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='sectionIII'></a>\n",
        "# **Section III**"
      ],
      "metadata": {
        "id": "-JzdR1PTCD4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import requests\n",
        "#@markdown We wish to share your experiences. <br>Send a report with suggestions and observations or share the Colab notebook with marco.luppichini@unifi.it. We will be happy to answer you.\n",
        "name = \"\"#@param {type:\"string\"}\n",
        "last_name =  \"\"#@param {type:\"string\"}\n",
        "email = \"\"#@param {type:\"string\"}\n",
        "affiliation = \"\"#@param {type:\"string\"}\n",
        "text = \"\"#@param {type:\"string\"}\n",
        "\n",
        "date = datetime.datetime.today()\n",
        "payload = {'name': name,\n",
        "           'last_name': last_name,\n",
        "           'email': email,\n",
        "           'text': text,\n",
        "           \"date\" : date,\n",
        "           \"affiliation\": affiliation}\n",
        "\n",
        "res = requests.post(\"http://131.114.22.25/CleverRiver/dist/php/sendInfo.php\", data = payload)\n",
        "if res.content == '':\n",
        "  print (\"Report sent\")\n",
        "else:\n",
        "  print (\"error\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "RboYmJGbCIct"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}